{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepsphere import HealpyGCNN\n",
    "from deepsphere import healpy_layers as hp_layer\n",
    "from deepsphere import utils\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyccl as ccl\n",
    "\n",
    "# cosmo = ccl.Cosmology(Omega_c = 0.262, Omega_b = 0.048, h = 0.69, sigma8 = 0.83, n_s = 0.96)\n",
    "\n",
    "# zs=np.linspace(0,1,100)\n",
    "# dNdz = np.ones(zs.shape)\n",
    "# bias = 1*np.ones(len(dNdz))\n",
    "\n",
    "# clustering = ccl.NumberCountsTracer(cosmo, has_rsd=False, dndz=(zs,dNdz), bias=(zs,bias))\n",
    "# ell = np.arange(0,6144)\n",
    "# cls_clu = ccl.angular_cl(cosmo, clustering, clustering, ell) #Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kSZMap = hp.read_map(\"./kSZ_NS_2048_R_2048_P_2560_DV_256.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kSZPowSpec = hp.anafast(kSZMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kSZFiles = os.listdir('./Data/kSZ')\n",
    "velFiles = os.listdir('./Data/velocityField')\n",
    "densFiles = os.listdir('./Data/overdensity')\n",
    "\n",
    "nside = 64\n",
    "numSets=len(densFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overdensData = np.zeros((numSets,12*nside**2))\n",
    "kSZData = np.zeros((numSets,12*nside**2))\n",
    "velData = np.zeros((numSets,12*nside**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,numSets):\n",
    "    overdensData[i] = hp.reorder(np.load('./Data/overdensity/'+densFiles[i]),r2n=True)\n",
    "    overdensData[i] = 100*overdensData[i]/np.linalg.norm(overdensData[i])\n",
    "    kSZData[i] = hp.reorder(np.load('./Data/kSZ/'+kSZFiles[i]),r2n=True)\n",
    "    kSZData[i] = 100*kSZData[i]/np.linalg.norm(kSZData[i])\n",
    "    velData[i] = hp.reorder(np.load('./Data/velocityField/'+velFiles[i]),r2n=True)\n",
    "    velData[i] = 100*velData[i]/np.linalg.norm(velData[i])\n",
    "# for i in range(0,numSets):\n",
    "#     overdensData[i] = hp.reorder(hp.synfast(cls_clu,nside=nside),r2n=True)\n",
    "#     kSZData[i] = hp.reorder(100000*hp.synfast(kSZPowSpec,nside=nside),r2n=True)\n",
    "#     velData[i] = hp.reorder(kSZData[i]/(overdensData[i]+1),r2n=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = np.stack((overdensData,kSZData),axis=2)\n",
    "x_raw = np.reshape(x_raw,(numSets,12*nside**2,2))\n",
    "\n",
    "x_train, x_test = np.split(x_raw, indices_or_sections=[numSets-numSets//20])\n",
    "y_train, y_test = np.split(velData, indices_or_sections=[numSets-numSets//20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_test = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_test),\n",
    "                                 tf.data.Dataset.from_tensor_slices(y_test)))\n",
    "dset_test = dset_test.shuffle(20)\n",
    "dset_test = dset_test.batch(8)\n",
    "\n",
    "dset_train = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_train),\n",
    "                                  tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "dset_train = dset_train.shuffle(200)\n",
    "dset_train = dset_train.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dset_train = dset_train.prefetch(buffer_size=AUTOTUNE)\n",
    "dset_test = dset_test.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(overdensData[350]/np.linalg.norm(overdensData[350]),nest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(kSZData[350]/np.linalg.norm(kSZData[350]),nest=True,max=.01,min=-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(velData[350]/np.linalg.norm(velData[350]),nest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "layersBest = [#hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=64, activation=\"linear\"),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=64, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=16, activation=\"elu\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"elu\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  hp_layer.HealpyMonomial(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #tf.keras.layers.BatchNormalization(),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "#                   hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "#                                    activation=\"linear\"),\n",
    "                  #tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                           activation=\"linear\"),\n",
    "                  tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPool(p=1,pool_type='MAX'),\n",
    "                  hp_layer.HealpyMonomial(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=16, activation=\"elu\"),\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=16, activation=\"elu\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  #tf.keras.layers.Dense(12*nside**2)\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=24, activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=24, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=16, use_bias=True, use_bn=False, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=8, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=1, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\")\n",
    "         ]\n",
    "\n",
    "layersTest = [#hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=64, activation=\"linear\"),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=64, use_bias=True, use_bn=False, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=16, activation=\"elu\"),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"elu\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  hp_layer.HealpyMonomial(K=K, Fout=24, use_bias=True, use_bn=True, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyPseudoConv(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #tf.keras.layers.BatchNormalization(),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  #tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                           activation=\"linear\"),\n",
    "                  tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPool(p=1,pool_type='MAX'),\n",
    "                  hp_layer.HealpyMonomial(K=K, Fout=32, use_bias=True, use_bn=True, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=32, activation=\"linear\"),\n",
    "                  #tf.keras.layers.LayerNormalization(axis=1),\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=16, activation=\"elu\"),\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=16, activation=\"elu\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=24, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  #tf.keras.layers.Dense(12*nside**2)\n",
    "                  #hp_layer.HealpyPseudoConv_Transpose(p=1, Fout=24, activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=24, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=24, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=16, use_bias=True, use_bn=True, \n",
    "                                           activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=8, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\"),\n",
    "                  hp_layer.HealpyChebyshev(K=K, Fout=1, use_bias=True, use_bn=True, \n",
    "                                   activation=\"linear\")\n",
    "         ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(hp_layer.HealpyChebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pix = hp.nside2npix(nside)\n",
    "indices = np.arange(n_pix)\n",
    "modelHP = HealpyGCNN(nside=nside, layers=layersBest,indices=np.arange(0,12*nside**2), n_neighbors=40)\n",
    "modelHP.build(input_shape=(None, len(indices), 2))\n",
    "modelHP.summary(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "modelHP.compile(optimizer=opt,\n",
    "              #loss=tf.keras.losses.Huber(delta=1.0,reduction=\"auto\",name=\"huber_loss\"),\n",
    "                loss=\"MSE\",\n",
    "              metrics=[r_square]\n",
    ")\n",
    "\n",
    "checkpoint_filepath = './checkpoint'\n",
    "\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath+\"/{epoch}\",\n",
    "#     save_weights_only=False,\n",
    "#     monitor='loss',\n",
    "#     save_freq=3\n",
    "# )\n",
    "filePath = \"./models2/Model1_weights.{epoch:02d}.hdf5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filePath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 20\n",
    "history = modelHP.fit(\n",
    "    dset_train,\n",
    "    epochs=initial_epochs,\n",
    "    validation_data = dset_test,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 2\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_further = modelHP.fit(dset_train,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs =  initial_epochs + fine_tune_epochs+1\n",
    "\n",
    "history_furthest = modelHP.fit(dset_train,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history_further.epoch[-1],\n",
    "                         validation_data=dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs =  initial_epochs + fine_tune_epochs+1+1\n",
    "\n",
    "history_furthester = modelHP.fit(dset_train,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history_furthest.epoch[-1],\n",
    "                         validation_data=dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHP.save('SavedModels/BestModelSoFar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = np.reshape(modelHP(x_train[10:11],True).numpy(),(12*nside**2))\n",
    "goal = np.reshape(y_train[10:11],(12*nside**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp.mollview(goal, nest=True, title=\"Goal Map\");\n",
    "hp.mollview(reconst, nest=True, title=\"Reconstruction Map\");\n",
    "hp.mollview(reconst - goal, nest=True, title=\"Difference\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = hp.reorder(reconst,n2r=True)\n",
    "goal = hp.reorder(goal,n2r=True)\n",
    "plt.loglog(hp.anafast(reconst))\n",
    "plt.loglog(hp.anafast(goal))\n",
    "plt.legend([\"Reconstructed\",\"Actual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = hp.anafast(reconst,goal)/np.sqrt(hp.anafast(goal)*hp.anafast(reconst)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(correlation);\n",
    "plt.grid()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.concatenate((history.history['loss'],history_further.history['loss'],history_furthest.history['loss'])), label='loss');\n",
    "plt.semilogy(np.concatenate((history.history['val_loss'],history_further.history['val_loss'],history_furthest.history['val_loss'])), label = 'val_loss');\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim([0.5, 1])\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.append(np.append(history.history['r_square'],history_further.history['r_square']),history_furthest.history['r_square']), label='r_square')\n",
    "plt.semilogy(np.append(np.append(history.history['val_r_square'],history_further.history['val_r_square']),history_furthest.history['val_r_square']), label = 'val_r_square')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim([0.5, 1])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstAll = np.reshape(modelHP(x_train,False).numpy(),(12*nside**2))\n",
    "goal = np.reshape(y_train,(12*nside**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHP(x_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHP.save(\"./SavedModels/R2.81/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
